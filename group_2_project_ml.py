# -*- coding: utf-8 -*-
"""GROUP 2 PROJECT ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EQ-BzM8Y3Tby5Q6tsiA-AUA9Az7nmIIy

# **IMPORT DATASETS**
"""

import pandas as pd
enrolment = pd.read_csv("enrolment_school_district.csv")
completion = pd.read_csv("completion_school_state.csv")
income = pd.read_csv("hh_income_district.csv")
poverty = pd.read_csv("hh_poverty_state.csv")

"""# **DATA UNDERSTANDING**"""

print(enrolment.head())
print(enrolment.columns)
print(enrolment.dtypes)

print(completion.head())
print(completion.columns)
print(completion.dtypes)

print(income.head())
print(income.columns)
print(income.dtypes)

print(poverty.head())
print(poverty.columns)
print(poverty.dtypes)

"""# **DATA PREPARATION**

### **CHECK FOR MISSING AND DUPLICATES VALUE**
"""

enrolment.isnull().sum()

enrolment.duplicated().sum()

completion.isnull().sum()

completion.duplicated().sum()

income.isnull().sum()

income.duplicated().sum()

poverty.isnull().sum()

poverty.duplicated().sum()

"""### **HANDLING THE MISSING AND DUPLICATES DATA**"""

enrolment = enrolment.drop_duplicates()

poverty['poverty_absolute'] = poverty['poverty_absolute'].fillna(poverty['poverty_absolute'].mean())
poverty['poverty_hardcore'] = poverty['poverty_hardcore'].fillna(poverty['poverty_hardcore'].mean())
poverty['poverty_relative'] = poverty['poverty_relative'].fillna(poverty['poverty_relative'].mean())

"""### **CHECK THE MISSING AND DUPLICATES DATA AFTER CLEANING**"""

enrolment.duplicated().sum()

# Save the final cleaned data to CSV
output_filename = 'cleaned_enrolment.csv'
enrolment.to_csv(output_filename, index=False)

poverty.isnull().sum()

# Save the final cleaned data to CSV
output_filename = 'cleaned_poverty.csv'
poverty.to_csv(output_filename, index=False)

"""### **FEATURE ENGINEERING**"""

import pandas as pd

enrolment = pd.read_csv("cleaned_enrolment.csv")
poverty = pd.read_csv("cleaned_poverty.csv")

# Drop 'sex' and 'district' in the enrolment to sync all the dataset
enrolment = enrolment.drop(columns=["sex", "district"], errors='ignore')

# Drop 'district' and 'income_median' column in income
income = income.drop(columns=["district", "income_median"], errors='ignore')

# Drop 'sex' cloumn in completion
completion = completion.drop(columns=["sex"], errors='ignore')

# Ensure all 'date' columns are datetime objects before merging
enrolment['date'] = pd.to_datetime(enrolment['date'])
completion['date'] = pd.to_datetime(completion['date'])
income['date'] = pd.to_datetime(income['date'])
poverty['date'] = pd.to_datetime(poverty['date'])

#Combine enrolment and completion data based on on 'state', 'date', and 'stage'.
education = pd.merge(
    enrolment,
    completion,
    on=['state', 'date', 'stage'],
    how='outer',
    suffixes=('_enrolment', '_completion')
)

#Combine theincome data on 'state' and 'date'.
education_income = pd.merge(
    education,
    income,
    on=['state', 'date'],
    how='left' # Use 'left' to keep all education rows
)

# 3. Merge the result with poverty data on 'state' and 'date'.
merged_df = pd.merge(
    education_income,
    poverty,
    on=['state', 'date'],
    how='left' # Use 'left' to keep all existing rows
)

# Save the final merged DataFrame to a new CSV file
merged_df.to_csv("merged_df.csv", index=False)

"""### **FEATURE SELECTION**"""

import pandas as pd

df = pd.read_csv("merged_df.csv")

df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year

X = df.drop(columns=['completion', 'date'])
y = df['completion']

X = pd.get_dummies(X, columns=['state', 'stage'], drop_first=True)

print("Prepared features head:")
display(X.head())
print(f"\nShape of prepared features: {X.shape}")

"""### **CHECK THE NEW DATASET NULL, DUPLICATES AND OUTLIERS**

"""

data = pd.read_csv("merged_df.csv")

data.isnull().sum()

data.duplicated().sum()

# Check the outliers before cleaning
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 5))
sns.boxplot(y=data['students'].dropna())
plt.yscale('log')
plt.title('Distribution of Student Counts Before Cleaning', fontsize=14)
plt.ylabel('Students', fontsize=12)

"""### **HANDLE THE DUPLICATES, NULL VALUE**"""

data_cleaned = data.drop_duplicates()

# Handle Null Values using Median Imputation
numerical_nulls = data_cleaned.select_dtypes(include=['float64', 'int64']).columns[data_cleaned.select_dtypes(include=['float64', 'int64']).isnull().any()]

# Create a copy to avoid SettingWithCopyWarning
data_imputed = data_cleaned.copy()

# Impute nulls with the median for each column
for col in numerical_nulls:
    median_val = data_imputed[col].median()
    data_imputed[col].fillna(median_val, inplace=True)

# Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = data_imputed['students'].quantile(0.25)
Q3 = data_imputed['students'].quantile(0.75)

# Calculate the Interquartile Range (IQR)
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out the outliers to create a new DataFrame
data_imputed_no_outliers = data_imputed[(data_imputed['students'] >= lower_bound) & (data_imputed['students'] <= upper_bound)].copy()

"""### **CHECK THE DATASET AFTER CLEANING**"""

data_imputed_no_outliers.isnull().sum()

data_imputed_no_outliers.duplicated().sum()

#Outlier Detection for 'students' using Box Plot (After Cleaning)
plt.figure(figsize=(8, 5))
sns.boxplot(y=data_imputed_no_outliers['students'])
plt.yscale('log')
plt.title('Distribution of Student Counts After Cleaning', fontsize=14)
plt.ylabel('Students', fontsize=12)

# FINAL DATASET
final_file_name = "final_data.csv"
data_imputed.to_csv(final_file_name, index=False)

"""**EXPLORATORY DATA ANALYSIS (EDA)**"""

import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("final_data.csv")
df

#remove post secondary bcs bias
df = df.drop(columns=['post_secondary'],errors='ignore')
df.describe()

#problem: student enrolement 0 but completion rate 100
ghost_rows = df[df['students'] == 0]
ghost_rows.head()

#only keep student > 1
df = df[df['students'] > 0]

df.describe()

"""Viz 1 : Education Journey

Shows how completion rates become unstable as education level increases.
"""

stage_order = ['primary', 'secondary_lower', 'secondary_upper']

plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='stage', y='completion', order=stage_order, palette='Set2')

plt.title('Distribution of Completion Rates by Education Level')
plt.ylabel('Completion Rate (%)')
plt.xlabel('Education Level')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""Median: High median = System works for most.

Box Height (IQR): Variance/Inequality.

--Short box = Equal outcomes.

--Tall box = Outcomes depend on location.

Outliers: Specific states failing significantly below the norm.

1. PRIMARY EDUCATION

Insight: Highly compressed box with minimal height.

Meaning: "High Consistency." Almost every state performs equally well.

Conclusion: Primary education is universal; location does not affect success.

2. LOWER SECONDARY

Insight: Box is still compact, but outliers appear below the whisker.

Meaning: The average is still good, but specific schools start failing.

Conclusion: Dropouts don't happen suddenly, the retention issues start here.

3. UPPER SECONDARY


Insight: Tallest box (High Variance) with the lowest outliers (<85%).

Meaning: High Inequality. The gap between best and worst states is massive.

Conclusion: This is the critical problem area. Success depends heavily on the state.

Viz 2 : State vs Education Level

Which state fail at which level?
"""

heatmap_data = df.groupby(['state', 'stage'])['completion'].mean().unstack()

heatmap_data = heatmap_data.loc[heatmap_data.mean(axis=1).sort_values(ascending=False).index]
heatmap_data = heatmap_data[stage_order]

plt.figure(figsize=(10, 8))
sns.heatmap(heatmap_data, annot=True, fmt=".1f", cmap="RdYlGn", center=95)
plt.title('Average Completion Rate by State and Education Level')
plt.show()

"""1. Primary level
- High Performers: Terengganu.

This state has a very high reliance on the public school system. Unlike KL, there are fewer private or international schools in rural areas. Value above 100% maybe because migration (families moving back to their hometown)

- Lowest:  W.P. Kuala Lumpur and Labuan

Wealthy urban parents in KL and the financial hub of Labuan often send their children to private schools, international schools or homeschooling centers

2. Secondary level
- Highest: W.P Putrajaya

Many civil servants  moving there for work. They bring their families with them, causing a massive influx of students in the middle of the school cycle.

- Lowest: W.P Labuan

Labuan is a small island economy. The lower rate persists likely due to students transferring out to boarding schools on the mainland.


3. Secondary upper
- Highest: Kelantan
-Lowest: W.P Kuala Lumpur

Viz 3 : Impact of Income on School Completion
"""

sns.lmplot(
    data=df,
    x='income_mean',
    y='completion',
    hue='stage',
    col='stage',
    col_order=stage_order,
    col_wrap=2,
    height=4,
    aspect=1.2,
    scatter_kws={'alpha': 0.3}
)
plt.suptitle("Impact of Income on Completion Rate Across Different Education Level", y=1.02)
plt.show()

"""- Primary: The regression line is nearly flat.
- Secondary levels: A positive line. As mean income increases, completion rates tend to rise.Income affect the completion rate.

Viz 4 : Trend of School Completion Over Time
"""

df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year

plt.figure(figsize=(12, 6))
sns.lineplot(
    data=df,
    x='year',
    y='completion',
    hue='stage',
    style='stage',
    hue_order=stage_order,
    style_order=stage_order,
    markers=True,
    dashes=False,
    palette='Set2'
)

plt.title('Completion Rate Trends Over Time (2016 - 2022)')
plt.ylabel('Completion Rate (%)')
plt.xlabel('Year')
plt.legend(title='Education Level')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""- Blue line for secondary student in year 2020 shows a very sharp drop downward, indicates that the education system faced a major crisis and high school students were the most affected by lockdowns.
- After pandemic (2021-2022), the trend shows an upward trend which means the education system improved significantly once physical classes resumed.

MACHINE LEARNING
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

df = pd.read_csv("final_data.csv")

target = "completion"

X = df.drop(columns=[target])
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Identify numerical and categorical columns dynamically
numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

# Create a ColumnTransformer to apply different transformations to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Apply the preprocessor to X_train and X_test
X_train_scaled = preprocessor.fit_transform(X_train)
X_test_scaled = preprocessor.transform(X_test)

def evaluate_model(y_true, y_pred):
    return {
        "R2": r2_score(y_true, y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "MAE": mean_absolute_error(y_true, y_pred)
    }

results = {}

"""MULTIPLE LINEAR REGRESSION"""

from sklearn.linear_model import LinearRegression

# 1. Initialize and Fit
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

# 2. Dual Predictions
pred_lr_train = lr.predict(X_train_scaled)
pred_lr_test = lr.predict(X_test_scaled)

# 3. Comprehensive Evaluation
train_r2_lr = r2_score(y_train, pred_lr_train)
test_r2_lr = r2_score(y_test, pred_lr_test)

results["Multiple Linear Regression"] = {
    "Train R2": train_r2_lr,
    "Test R2": test_r2_lr,
    "Test RMSE": np.sqrt(mean_squared_error(y_test, pred_lr_test)),
    "Test MAE": mean_absolute_error(y_test, pred_lr_test),
    "Overfit Gap": train_r2_lr - test_r2_lr
}

"""DECISION TREE"""

from sklearn.tree import DecisionTreeRegressor

# 1. Initialize and Fit
dt = DecisionTreeRegressor(random_state=42)
dt.fit(X_train_scaled, y_train)

# 2. Dual Predictions
pred_dt_train = dt.predict(X_train_scaled)
pred_dt_test = dt.predict(X_test_scaled)

# 3. Comprehensive Evaluation
train_r2_dt = r2_score(y_train, pred_dt_train)
test_r2_dt = r2_score(y_test, pred_dt_test)

results["Decision Tree"] = {
    "Train R2": train_r2_dt,
    "Test R2": test_r2_dt,
    "Test RMSE": np.sqrt(mean_squared_error(y_test, pred_dt_test)),
    "Test MAE": mean_absolute_error(y_test, pred_dt_test),
    "Overfit Gap": train_r2_dt - test_r2_dt
}

"""RANDOM FOREST"""

from sklearn.ensemble import RandomForestRegressor

# 1. Initialize and Fit
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train_scaled, y_train)

# 2. Dual Predictions
pred_rf_train = rf.predict(X_train_scaled)
pred_rf_test = rf.predict(X_test_scaled)

# 3. Comprehensive Evaluation
train_r2_rf = r2_score(y_train, pred_rf_train)
test_r2_rf = r2_score(y_test, pred_rf_test)

results["Random Forest"] = {
    "Train R2": train_r2_rf,
    "Test R2": test_r2_rf,
    "Test RMSE": np.sqrt(mean_squared_error(y_test, pred_rf_test)),
    "Test MAE": mean_absolute_error(y_test, pred_rf_test),
    "Overfit Gap": train_r2_rf - test_r2_rf
}

"""K-NEAREST NEIGHBORS (KNN)"""

from sklearn.neighbors import KNeighborsRegressor

# 1. Initialize and Fit
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# 2. Dual Predictions
pred_knn_train = knn.predict(X_train_scaled)
pred_knn_test = knn.predict(X_test_scaled)

# 3. Comprehensive Evaluation
train_r2_knn = r2_score(y_train, pred_knn_train)
test_r2_knn = r2_score(y_test, pred_knn_test)

results["K-NN"] = {
    "Train R2": train_r2_knn,
    "Test R2": test_r2_knn,
    "Test RMSE": np.sqrt(mean_squared_error(y_test, pred_knn_test)),
    "Test MAE": mean_absolute_error(y_test, pred_knn_test),
    "Overfit Gap": train_r2_knn - test_r2_knn
}

"""EXTREME GRADIENT BOOSTING"""

from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

# 1. Initialize and Fit
xgb = XGBRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    random_state=42
)
xgb.fit(X_train_scaled, y_train)

# 2. Predict on BOTH sets to check for overfitting
pred_xgb_train = xgb.predict(X_train_scaled)
pred_xgb_test = xgb.predict(X_test_scaled)

# 3. Calculate Dual Metrics
train_r2 = r2_score(y_train, pred_xgb_train)
test_r2 = r2_score(y_test, pred_xgb_test)

# 4. Store Comprehensive Results
results["XGBoost"] = {
    "Train R2": train_r2,
    "Test R2": test_r2,
    "Test RMSE": np.sqrt(mean_squared_error(y_test, pred_xgb_test)),
    "Test MAE": mean_absolute_error(y_test, pred_xgb_test),
    "Overfit Gap": train_r2 - test_r2  # The critical scientific check
}

results_df = pd.DataFrame(results).T
results_df

"""VISUALIZATION OF MODEL PERFORMANCE"""

#ACTUAL VS PREDICTED PLOT FOR EACH MODELS
import matplotlib.pyplot as plt
import seaborn as sns

# Define a dictionary of your test predictions from the previous step
predictions = {
    "Multiple Linear Regression": pred_lr_test,
    "Decision Tree": pred_dt_test,
    "Random Forest": pred_rf_test,
    "K-NN": pred_knn_test,
    "XGBoost": pred_xgb_test
}

# Create a figure with a 2x3 grid (since you have 5 models)
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))
axes = axes.flatten() # Flatten to 1D for easy looping

for i, (name, pred) in enumerate(predictions.items()):
    ax = axes[i]

    # 1. Scatter plot of Actual vs Predicted
    sns.scatterplot(x=y_test, y=pred, alpha=0.5, ax=ax, color='blue')

    # 2. Add the "Perfect Prediction" Line (Red Dashed)
    line_coords = [y_test.min(), y_test.max()]
    ax.plot(line_coords, line_coords, 'r--', lw=2, label='Perfect Fit')

    # 3. Formatting
    ax.set_title(f"{name}\n(R2: {results[name]['Test R2']:.3f})")
    ax.set_xlabel("Actual Completion Rate (%)")
    ax.set_ylabel("Predicted Completion Rate (%)")
    ax.legend()

# Remove the empty 6th subplot
fig.delaxes(axes[5])

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

models = results_df.index
rmse = results_df['RMSE']
mae = results_df['MAE']

x = np.arange(len(models))
width = 0.35

plt.figure(figsize=(9,5))
plt.bar(x - width/2, rmse, width, label='RMSE')
plt.bar(x + width/2, mae, width, label='MAE')

plt.xlabel('Model')
plt.ylabel('Error Value')
plt.title('RMSE and MAE Comparison Across Regression Models')
plt.xticks(x, models)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

import matplotlib.pyplot as plt

# R² comparison
plt.figure(figsize=(8,5))
plt.bar(results_df.index, results_df['R2'])
plt.title('R² Score Comparison Across Regression Models')
plt.ylabel('R² Score')
plt.xlabel('Model')
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

"""Lower RMSE and MAE values indicate better prediction accuracy, while higher R² values indicate stronger model performance.

Multiple Linear Regression shows the weakest performance with the highest errors and lowest R².

Decision Tree and Random Forest improve prediction accuracy but remain less effective than XGBoost.

XGBoost achieves the lowest RMSE and MAE and the highest R², making it the best-performing model.
"""

residuals_df = pd.DataFrame({
    'MLR': y_test - pred_mlr,
    'Decision Tree': y_test - pred_dt,
    'Random Forest': y_test - pred_rf,
    'XGBoost': y_test - pred_xgb
})

plt.figure(figsize=(9,5))
residuals_df.boxplot()
plt.axhline(0, linestyle='--')
plt.ylabel('Residuals')
plt.title('Residual Comparison Across Models')
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
import pandas as pd
import matplotlib.pyplot as plt

# Select numerical features only
features = [
    'students',
    'income_mean',
    'poverty_absolute',
    'poverty_hardcore',
    'poverty_relative'
]

X = df[features]
y = df['completion']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit linear regression
lr = LinearRegression()
lr.fit(X_scaled, y)

# Get standardized coefficients
coef_df = pd.DataFrame({
    'Feature': features,
    'Coefficient': lr.coef_
})

# Sort by absolute importance
coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()
coef_df = coef_df.sort_values('Abs_Coefficient', ascending=True)

# Plot
plt.figure(figsize=(8,5))
plt.barh(coef_df['Feature'], coef_df['Abs_Coefficient'])
plt.xlabel('Standardized Influence on Completion Rate')
plt.title('Relative Influence of Socioeconomic Factors on Completion Rate')
plt.show()

"""Absolute poverty has the strongest influence on completion outcomes.

Hardcore poverty is the second most influential factor.

Relative poverty has a moderate impact compared to other poverty indicators.

Student population size shows minimal influence on completion rate.

Mean household income has the lowest influence among the selected variables.

Overall, poverty-related factors contribute more strongly to completion rate variations than income level or student numbers.
"""